\documentclass[11pt,addpoints,answers]{exam}

%-----------------------------------------------------------------------------
% PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%-----------------------------------------------------------------------------

\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsfonts}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{titling}
\usepackage{url}
\usepackage{xfrac}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{paralist}
\usepackage{epstopdf}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[colorlinks=true,urlcolor=blue]{hyperref}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{float}
\usepackage{enumerate}
\usepackage{array}
\usepackage{environ}
\usepackage{times}
\usepackage{textcomp}
\usepackage{caption}
\usepackage{parskip} % For NIPS style paragraphs.
\usepackage[compact]{titlesec} % Less whitespace around titles
\usepackage[inline]{enumitem} % For inline enumerate* and itemize*
\usepackage{datetime}
\usepackage{comment}
% \usepackage{minted}
\usepackage{lastpage}
\usepackage{color}
\usepackage{xcolor}
\usepackage[final]{listings}
\usepackage{tikz}
\usetikzlibrary{shapes,decorations}
\usepackage{framed}
\usepackage{booktabs}
\usepackage{cprotect}
\usepackage{verbatim}
\usepackage{verbatimbox}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{mathtools} % For drcases
\usepackage{cancel}
\usepackage[many]{tcolorbox}
\usepackage{soul}
\usepackage[bottom]{footmisc}
\usepackage{bm}
\usepackage{wasysym}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{positioning, arrows, automata, calc}
\usepackage{transparent}

\newtcolorbox[]{your_solution}[1][]{
    % breakable,
    enhanced,
    nobeforeafter,
    colback=white,
    title=Your Answer,
    sidebyside align=top,
    box align=top,
    #1
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Formatting for \CorrectChoice of "exam" %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\CorrectChoiceEmphasis{}
\checkedchar{\blackcircle}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Rotated Column Headers                  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{adjustbox}
\usepackage{array}

%https://tex.stackexchange.com/questions/32683/rotated-column-titles-in-tabular

\newcolumntype{R}[2]{%
    >{\adjustbox{angle=#1,lap=\width-(#2)}\bgroup}%
    l%
    <{\egroup}%
}
\newcommand*\rot{\multicolumn{1}{R{45}{1em}}}% no optional argument here, please!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Custom commands                        %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\vc}[1]{\boldsymbol{#1}}
\newcommand{\adj}[1]{\frac{d J}{d #1}}
\newcommand{\chain}[2]{\adj{#2} = \adj{#1}\frac{d #1}{d #2}}

\newcommand{\independent}{\perp\!\!\!\perp}

\newcommand{\R}{\mathbb{R}}
\newcommand{\blackcircle}{\tikz\draw[black,fill=black] (0,0) circle (1ex);}
\renewcommand{\circle}{\tikz\draw[black] (0,0) circle (1ex);}

\newcommand{\emptysquare}{{\LARGE $\square$}\ \ }
\newcommand{\filledsquare}{{\LARGE $\blacksquare$}\ \ }
\newcommand{\emptycircle}{{\LARGE $\fullmoon$}\ \ }
\newcommand{\filledcircle}{{\LARGE $\newmoon$}\ \ }

\newcommand{\ntset}{test}

% mathcal
\newcommand{\Ac}{\mathcal{A}}
\newcommand{\Bc}{\mathcal{B}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\Dc}{\mathcal{D}}
\newcommand{\Ec}{\mathcal{E}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Gc}{\mathcal{G}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\Ic}{\mathcal{I}}
\newcommand{\Jc}{\mathcal{J}}
\newcommand{\Kc}{\mathcal{K}}
\newcommand{\Lc}{\mathcal{L}}
\newcommand{\Mc}{\mathcal{M}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\Oc}{\mathcal{O}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Qc}{\mathcal{Q}}
\newcommand{\Rc}{\mathcal{R}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\Tc}{\mathcal{T}}
\newcommand{\Uc}{\mathcal{U}}
\newcommand{\Vc}{\mathcal{V}}
\newcommand{\Wc}{\mathcal{W}}
\newcommand{\Xc}{\mathcal{X}}
\newcommand{\Yc}{\mathcal{Y}}
\newcommand{\Zc}{\mathcal{Z}}

% mathbb
\newcommand{\Ab}{\mathbb{A}}
\newcommand{\Bb}{\mathbb{B}}
\newcommand{\Cb}{\mathbb{C}}
\newcommand{\Db}{\mathbb{D}}
\newcommand{\Eb}{\mathbb{E}}
\newcommand{\Fb}{\mathbb{F}}
\newcommand{\Gb}{\mathbb{G}}
\newcommand{\Hb}{\mathbb{H}}
\newcommand{\Ib}{\mathbb{I}}
\newcommand{\Jb}{\mathbb{J}}
\newcommand{\Kb}{\mathbb{K}}
\newcommand{\Lb}{\mathbb{L}}
\newcommand{\Mb}{\mathbb{M}}
\newcommand{\Nb}{\mathbb{N}}
\newcommand{\Ob}{\mathbb{O}}
\newcommand{\Pb}{\mathbb{P}}
\newcommand{\Qb}{\mathbb{Q}}
\newcommand{\Rb}{\mathbb{R}}
\newcommand{\Sb}{\mathbb{S}}
\newcommand{\Tb}{\mathbb{T}}
\newcommand{\Ub}{\mathbb{U}}
\newcommand{\Vb}{\mathbb{V}}
\newcommand{\Wb}{\mathbb{W}}
\newcommand{\Xb}{\mathbb{X}}
\newcommand{\Yb}{\mathbb{Y}}
\newcommand{\Zb}{\mathbb{Z}}

% mathbf lowercase
\newcommand{\av}{\mathbf{a}}
\newcommand{\bv}{\mathbf{b}}
\newcommand{\cv}{\mathbf{c}}
\newcommand{\dv}{\mathbf{d}}
\newcommand{\ev}{\mathbf{e}}
\newcommand{\fv}{\mathbf{f}}
\newcommand{\gv}{\mathbf{g}}
\newcommand{\hv}{\mathbf{h}}
\newcommand{\iv}{\mathbf{i}}
\newcommand{\jv}{\mathbf{j}}
\newcommand{\kv}{\mathbf{k}}
\newcommand{\lv}{\mathbf{l}}
\newcommand{\mv}{\mathbf{m}}
\newcommand{\nv}{\mathbf{n}}
\newcommand{\ov}{\mathbf{o}}
\newcommand{\pv}{\mathbf{p}}
\newcommand{\qv}{\mathbf{q}}
\newcommand{\rv}{\mathbf{r}}
\newcommand{\sv}{\mathbf{s}}
\newcommand{\tv}{\mathbf{t}}
\newcommand{\uv}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\wv}{\mathbf{w}}
\newcommand{\xv}{\mathbf{x}}
\newcommand{\yv}{\mathbf{y}}
\newcommand{\zv}{\mathbf{z}}

% mathbf uppercase
\newcommand{\Av}{\mathbf{A}}
\newcommand{\Bv}{\mathbf{B}}
\newcommand{\Cv}{\mathbf{C}}
\newcommand{\Dv}{\mathbf{D}}
\newcommand{\Ev}{\mathbf{E}}
\newcommand{\Fv}{\mathbf{F}}
\newcommand{\Gv}{\mathbf{G}}
\newcommand{\Hv}{\mathbf{H}}
\newcommand{\Iv}{\mathbf{I}}
\newcommand{\Jv}{\mathbf{J}}
\newcommand{\Kv}{\mathbf{K}}
\newcommand{\Lv}{\mathbf{L}}
\newcommand{\Mv}{\mathbf{M}}
\newcommand{\Nv}{\mathbf{N}}
\newcommand{\Ov}{\mathbf{O}}
\newcommand{\Pv}{\mathbf{P}}
\newcommand{\Qv}{\mathbf{Q}}
\newcommand{\Rv}{\mathbf{R}}
\newcommand{\Sv}{\mathbf{S}}
\newcommand{\Tv}{\mathbf{T}}
\newcommand{\Uv}{\mathbf{U}}
\newcommand{\Vv}{\mathbf{V}}
\newcommand{\Wv}{\mathbf{W}}
\newcommand{\Xv}{\mathbf{X}}
\newcommand{\Yv}{\mathbf{Y}}
\newcommand{\Zv}{\mathbf{Z}}

% bold greek lowercase
\newcommand{\alphav     }{\boldsymbol \alpha     }
\newcommand{\betav      }{\boldsymbol \beta      }
\newcommand{\gammav     }{\boldsymbol \gamma     }
\newcommand{\deltav     }{\boldsymbol \delta     }
\newcommand{\epsilonv   }{\boldsymbol \epsilon   }
\newcommand{\varepsilonv}{\boldsymbol \varepsilon}
\newcommand{\zetav      }{\boldsymbol \zeta      }
\newcommand{\etav       }{\boldsymbol \eta       }
\newcommand{\thetav     }{\boldsymbol \theta     }
\newcommand{\varthetav  }{\boldsymbol \vartheta  }
\newcommand{\iotav      }{\boldsymbol \iota      }
\newcommand{\kappav     }{\boldsymbol \kappa     }
\newcommand{\varkappav  }{\boldsymbol \varkappa  }
\newcommand{\lambdav    }{\boldsymbol \lambda    }
\newcommand{\muv        }{\boldsymbol \mu        }
\newcommand{\nuv        }{\boldsymbol \nu        }
\newcommand{\xiv        }{\boldsymbol \xi        }
\newcommand{\omicronv   }{\boldsymbol \omicron   }
\newcommand{\piv        }{\boldsymbol \pi        }
\newcommand{\varpiv     }{\boldsymbol \varpi     }
\newcommand{\rhov       }{\boldsymbol \rho       }
\newcommand{\varrhov    }{\boldsymbol \varrho    }
\newcommand{\sigmav     }{\boldsymbol \sigma     }
\newcommand{\varsigmav  }{\boldsymbol \varsigma  }
\newcommand{\tauv       }{\boldsymbol \tau       }
\newcommand{\upsilonv   }{\boldsymbol \upsilon   }
\newcommand{\phiv       }{\boldsymbol \phi       }
\newcommand{\varphiv    }{\boldsymbol \varphi    }
\newcommand{\chiv       }{\boldsymbol \chi       }
\newcommand{\psiv       }{\boldsymbol \psi       }
\newcommand{\omegav     }{\boldsymbol \omega     }

% bold greek uppercase
\newcommand{\Gammav     }{\boldsymbol \Gamma     }
\newcommand{\Deltav     }{\boldsymbol \Delta     }
\newcommand{\Thetav     }{\boldsymbol \Theta     }
\newcommand{\Lambdav    }{\boldsymbol \Lambda    }
\newcommand{\Xiv        }{\boldsymbol \Xi        }
\newcommand{\Piv        }{\boldsymbol \Pi        }
\newcommand{\Sigmav     }{\boldsymbol \Sigma     }
\newcommand{\Upsilonv   }{\boldsymbol \Upsilon   }
\newcommand{\Phiv       }{\boldsymbol \Phi       }
\newcommand{\Psiv       }{\boldsymbol \Psi       }
\newcommand{\Omegav     }{\boldsymbol \Omega     }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Code highlighting with listings         %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\definecolor{bluekeywords}{rgb}{0.13,0.13,1}
\definecolor{greencomments}{rgb}{0,0.5,0}
\definecolor{redstrings}{rgb}{0.9,0,0}
\definecolor{light-gray}{gray}{0.95}

\newcommand{\MYhref}[3][blue]{\href{#2}{\color{#1}{#3}}}%

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstdefinelanguage{Shell}{
  keywords={tar, cd, make},
  %keywordstyle=\color{bluekeywords}\bfseries,
  alsoletter={+},
  ndkeywords={python, py, javac, java, gcc, c, g++, cpp, .txt, octave, m, .tar},
  %ndkeywordstyle=\color{bluekeywords}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  %stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]",
  backgroundcolor = \color{light-gray}
}

\lstset{columns=fixed, basicstyle=\ttfamily,
    backgroundcolor=\color{light-gray},xleftmargin=0.5cm,frame=tlbr,framesep=4pt,framerule=0pt}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Custom box for highlights               %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Define box and box title style
\tikzstyle{mybox} = [fill=blue!10, very thick,
    rectangle, rounded corners, inner sep=1em, inner ysep=1em]

% \newcommand{\notebox}[1]{
% \begin{tikzpicture}
% \node [mybox] (box){%
%     \begin{minipage}{\textwidth}
%     #1
%     \end{minipage}
% };
% \end{tikzpicture}%
% }

\NewEnviron{notebox}{
\begin{tikzpicture}
\node [mybox] (box){
    \begin{minipage}{\textwidth}
        \BODY
    \end{minipage}
};
\end{tikzpicture}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Commands showing / hiding solutions     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% To HIDE SOLUTIONS (to post at the website for students), set this value to 0: 
\def\issoln{1}
% Some commands to allow solutions to be embedded in the assignment file.
\ifcsname issoln\endcsname \else \def\issoln{1} \fi
% Default to an empty solutions environ.
\NewEnviron{soln}{}{}
\if\issoln 1
% Otherwise, include solutions as below.
\RenewEnviron{soln}{
    \leavevmode\color{red}\ignorespaces
    % \textbf{Solution} \BODY
    \BODY
}{}
\fi

%% qauthor environment:
% Default to an empty qauthor environ.
\NewEnviron{qauthor}{}{}
%% To HIDE TAGS set this value to 0:
\def\showtags{0}
%%%%%%%%%%%%%%%%
\ifcsname showtags\endcsname \else \def\showtags{1} \fi
% Default to an empty tags environ.
\NewEnviron{tags}{}{}
\if\showtags 1
% Otherwise, include solutions as below.
\RenewEnviron{tags}{
    \fbox{
    \leavevmode\color{blue}\ignorespaces
    \textbf{TAGS:} \texttt{\url{\BODY}}
    }
    \vspace{-.5em}
}{}
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Commands for customizing the assignment %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\courseName}{10-301/10-601 Introduction to Machine Learning (Summer 2022)}
\newcommand{\hwName}{Homework 7: Graphical Models}
\newcommand{\dueDate}{Wednesday, July 27 at 1:00 PM}


\title{\textsc{\hwName}
%\thanks{Compiled on \today{} at \currenttime{}}
} % Title


\author{\courseName\\
\url{https://www.cs.cmu.edu/~hchai2/courses/10601/} \\
OUT: Wednesday, July 20 \\
DUE: \dueDate{} \\ 
TAs: Sana, Brendon, Ayush, Boyang (Jack), Chu
}

\newcommand{\homeworktype}{\string written/programming}

\date{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Useful commands for typesetting the questions %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand \expect {\mathbb{E}}
\newcommand \mle [1]{{\hat #1}^{\rm MLE}}
\newcommand \map [1]{{\hat #1}^{\rm MAP}}
\newcommand \argmax {\operatorname*{argmax}}
\newcommand \argmin {\operatorname*{argmin}}
\newcommand \code [1]{{\tt #1}}
\newcommand \datacount [1]{\#\{#1\}}
\newcommand \ind [1]{\mathbb{I}\{#1\}}

%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document configuration %
%%%%%%%%%%%%%%%%%%%%%%%%%%

% Don't display a date in the title and remove the white space
\predate{}
\postdate{}
\date{}

% Don't display an author and remove the white space
%\preauthor{}
%\postauthor{}

% Solo and group questions
\newcommand{\solo}{\textbf{[SOLO]} }
\newcommand{\group}{\textbf{[GROUP]} }

% Question type commands
\newcommand{\sall}{\textbf{Select all that apply: }}
\newcommand{\sone}{\textbf{Select one: }}
\newcommand{\tf}{\textbf{True or False: }}

% AdaBoost commands
\newcommand{\trainerr}[1]{\hat{\epsilon}_S \left(#1\right)}
\newcommand{\generr}[1]{\epsilon \left(#1\right)}
\newcommand{\D}{\mathcal{D}}
\newcommand{\margin}{\text{margin}}
\newcommand{\sign}{\text{sign}}
\newcommand{\PrS}{\hat{\Pr_{(x_i, y_i) \sim S}}}
\newcommand{\PrSinline}{\hat{\Pr}_{(x_i, y_i) \sim S}}  % inline PrS

% Abhi messing around with examdoc
\qformat{\textbf{{\Large \thequestion \; \; \thequestiontitle \ (\totalpoints \ points)}} \hfill}
\renewcommand{\thequestion}{\arabic{question}}
\renewcommand{\questionlabel}{\thequestion.}

\renewcommand{\thepartno}{\arabic{partno}}
\renewcommand{\partlabel}{\thepartno.}
\renewcommand{\partshook}{\setlength{\leftmargin}{0pt}}

\renewcommand{\thesubpart}{\alph{subpart}}
\renewcommand{\subpartlabel}{(\thesubpart)}

\renewcommand{\thesubsubpart}{\roman{subsubpart}}
\renewcommand{\subsubpartlabel}{\thesubsubpart.}

% copied from stack overflow, as all good things are
\newcommand\invisiblesection[1]{%
  \refstepcounter{section}%
  \addcontentsline{toc}{section}{\protect\numberline{\thesection}#1}%
  \sectionmark{#1}}

% quite possibly the worst workaround i have made for this class
\newcommand{\sectionquestion}[1]{
\titledquestion{#1}
\invisiblesection{#1}
~\vspace{-1em}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% New Environment for Pseudocode          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Python style for highlighting
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
morekeywords={self},              % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false
}}


% Python environment
\lstnewenvironment{your_code_solution}[1][]
{
\pythonstyle
\lstset{#1}
}
{}


%%%%%%%%%%%%%%%%%%
% Begin Document %
%%%%%%%%%%%%%%%%%% 

\begin{document}

\maketitle

\begin{notebox}
\paragraph{Summary} In this assignment you will implement a new named entity recognition system using Hidden Markov Models. You will begin by going through some multiple choice and short answer warm-up problems to build your intuition for these models and then use that intuition to build your own HMM models.
\end{notebox}\newcommand \maxsubs {10 }
\section*{START HERE: Instructions}
\begin{itemize}

\item \textbf{Collaboration Policy}: Please read the collaboration policy here: \url{https://www.cs.cmu.edu/~hchai2/courses/10601/#Syllabus}

\item\textbf{Late Submission Policy:} See the late submission policy here: \url{https://www.cs.cmu.edu/~hchai2/courses/10601/#Syllabus}

\item\textbf{Submitting your work:} You will use Gradescope to submit
  answers to all questions\ifthenelse{\equal{\homeworktype}{\string written}}{}{ and code}. Please
  follow instructions at the end of this PDF to correctly submit all your code to Gradescope.

  \begin{itemize}
    
 % COMMENT IF NOT USING CANVAS
\begin{comment}
  \item \textbf{Canvas:} Canvas (\url{https://canvas.cmu.edu}) will be
    used for quiz-style problems (e.g. multiple choice, true / false,
    numerical answers). Grading is done automatically.
    %
    You may only \textbf{submit once} on canvas, so be sure of your
    answers before you submit. However, canvas allows you to work on
    your answers and then close out of the page and it will save your
    progress.  You will not be granted additional submissions, so
    please be confident of your solutions when you are submitting your
    assignment.
    %
    {\color{red} The above is true for future assignments, but this one
    allows {\bf unlimited submissions}.}
\end{comment}
    
  % COMMENT IF NOT USING GRADESCOPE
   \item \textbf{Written:} For written problems such as short answer, multiple choice, derivations, proofs, or plots, please use the provided template. Submissions can be handwritten onto the template, but should be labeled and clearly legible. If your writing is not legible, you will not be awarded marks. Alternatively, submissions can be written in LaTeX. Each derivation/proof should be completed in the boxes provided. You are responsible for ensuring that your submission contains exactly the same number of pages and the same alignment as our PDF template. If you do not follow the template, your assignment may not be graded correctly by our AI assisted grader.

  %   COMMENT IF NOT USING GRADESCOPE AUTOGRADER
  \ifthenelse{\equal{\homeworktype}{\string written}}{}{
\item \textbf{Programming:} You will submit your code for programming questions on the homework to Gradescope (\url{https://gradescope.com}). After uploading your code, our grading scripts will autograde your assignment by running your program on a virtual machine (VM). When you are developing, check that the version number of the programming language environment (Python 3.9.6) and versions of permitted libraries (\texttt{numpy} 1.21.2) match those used on Gradescope. You have \maxsubs free Gradescope programming submissions. After \maxsubs submissions, you will begin to lose points from your total programming score. We recommend debugging your implementation locally first before submitting your code to Gradescope.}

  \end{itemize}
  
\ifthenelse{\equal{\homeworktype}{\string written}}{}{\item\textbf{Materials:} The data and reference output that you will need in order to complete this assignment is posted along with the writeup and template on the course website.}

\end{itemize}


%\ifthenelse{\equal{\homeworktype}{\string written}}{}{\begin{notebox}
%\paragraph{Linear Algebra Libraries} When implementing machine learning algorithms, it is often convenient to have a linear algebra library at your disposal. In this assignment, Java users may use EJML\footnote{\url{https://ejml.org}} or ND4J\footnote{\url{https://javadoc.io/doc/org.nd4j/nd4j-api/latest/index.html}} and C++ users may use Eigen\footnote{\url{http://eigen.tuxfamily.org/}}. Details below. 
%
%(As usual, Python users have NumPy.)
%
%\begin{description}
%\item[EJML for Java] EJML is a pure Java linear algebra package with three interfaces. We strongly recommend using the SimpleMatrix interface. The autograder will use EJML version 0.41. When compiling and running your code, we will add the additional command line argument {\footnotesize{\lstinline{-cp "linalg_lib/ejml-v0.41-libs/*:linalg_lib/nd4j-v1.0.0-M1.1-libs/*:./"}}}
%to ensure that all the EJML jars are on the classpath as well as your code. 

%\item[ND4J for Java] ND4J is a library for multidimensional tensors with an interface akin to Python's NumPy. The autograder will use ND4J version 1.0.0-M1.1. When compiling and running your code, we will add the additional command line argument {\footnotesize{\lstinline{-cp "linalg_lib/ejml-v0.41-libs/*:linalg_lib/nd4j-v1.0.0-M1.1-libs/*:./"}}} to ensure that all the ND4J jars are on the classpath as well as your code. 

%\item[Eigen for C++] Eigen is a header-only library, so there is no linking to worry about---just \lstinline{#include} whatever components you need. The autograder will use Eigen version 3.4.0. The command line arguments above demonstrate how we will call you code. When compiling your code we will include, the argument \lstinline{-I./linalg_lib} in order to include the \lstinline{linalg_lib/Eigen} subdirectory, which contains all the headers.

%\end{description} 
%We have included the correct versions of EJML/ND4J/Eigen in the \lstinline{linalg_lib.zip} posted on the Coursework page of the course website for your convenience. It contains the same \lstinline{linalg_lib/} directory that we will include in the current working directory when running your tests. Do {\bf not} include EJML, ND4J, or Eigen in your homework submission; the autograder will ensure that they are in place. 
%\end{notebox}}\clearpage

\section*{Instructions for Specific Problem Types}

For ``Select One" questions, please fill in the appropriate bubble completely:

\begin{quote}
\textbf{Select One:} Who taught this course?
    \begin{checkboxes}
     \CorrectChoice Henry Chai
     \choice Marie Curie
     \choice Noam Chomsky
    \end{checkboxes}
\end{quote}

If you need to change your answer, you may cross out the previous answer and bubble in the new answer:

\begin{quote}
\textbf{Select One:} Who taught this course?
    {
    \begin{checkboxes}
     \CorrectChoice Henry Chai
     \choice Marie Curie \checkboxchar{\xcancel{\blackcircle}{}}
     \choice Noam Chomsky
    \end{checkboxes}
    }
\end{quote}

For ``Select all that apply" questions, please fill in all appropriate squares completely:

\begin{quote}
\textbf{Select all that apply:} Which are scientists?
    {%
    \checkboxchar{$\Box$} \checkedchar{$\blacksquare$} % change checkbox style locally
    \begin{checkboxes}
    \CorrectChoice Stephen Hawking 
    \CorrectChoice Albert Einstein
    \CorrectChoice Isaac Newton
    \choice I don't know
    \end{checkboxes}
    }
\end{quote}

Again, if you need to change your answer, you may cross out the previous answer(s) and bubble in the new answer(s):

\begin{quote}
\textbf{Select all that apply:} Which are scientists?
    {%
    \checkboxchar{\xcancel{$\blacksquare$}} \checkedchar{$\blacksquare$} % change checkbox style locally
    \begin{checkboxes}
    \CorrectChoice Stephen Hawking 
    \CorrectChoice Albert Einstein
    \CorrectChoice Isaac Newton
    \choice I don't know
    \end{checkboxes}
    }
\end{quote}

For questions where you must fill in a blank, please make sure your final answer is fully included in the given space. You may cross out answers or parts of answers, but the final answer must still be within the given space.

\begin{quote}
\textbf{Fill in the blank:} What is the course number?

\begin{tcolorbox}[fit,height=1cm, width=4cm, blank, borderline={1pt}{-2pt},nobeforeafter]
    \begin{center}\huge10-601\end{center}
    \end{tcolorbox}\hspace{2cm}
    \begin{tcolorbox}[fit,height=1cm, width=4cm, blank, borderline={1pt}{-2pt},nobeforeafter]
    \begin{center}\huge10-\xcancel{6}301\end{center}
    \end{tcolorbox}
\end{quote}

\clearpage
{\LARGE \bf Written Questions (\numpoints \ points)} 
\begin{questions}
\sectionquestion{\LaTeX{} Bonus Point}
\label{sec:latex}

\begin{parts}
    \part[1] \sone Did you use \LaTeX{} for the entire written portion of this homework?
    
    \begin{checkboxes}
        % YOUR ANSWER
        % Change \choice to \CorrectChoice for the appropriate selection/selections 
        \choice Yes
        \choice No
    \end{checkboxes}
\end{parts}\sectionquestion{Bayesian Networks}

Consider the joint distribution over the binary random variables $X_1, X_2, X_3, X_4, X_5$ represented by the Bayesian Network shown in the figure.

% Modified from HW9 Bayesian Network
\begin{center}
\begin{tikzpicture}
\begin{scope}[every node/.style={circle,thick,draw}]
    \node (A) at (0,0) {$X_1$};
    \node (B) at (0,-1.5) {$X_2$};
    \node (C) at (-1.5, -3) {$X_3$};
    \node (D) at (1.5, -3) {$X_4$};
    \node (E) at (0, -4.5) {$X_5$};
\end{scope}

\begin{scope}[>={Stealth[black]},
              every node/.style={fill=white,circle},
              every edge/.style={draw=black,very thick}]
    \path [->] (A) edge (B);
    \path [->] (B) edge (C);
    \path [->] (B) edge (D);
    \path [->] (C) edge (E);
    \path [->] (D) edge (E);
\end{scope}
\end{tikzpicture}
\end{center}

\begin{parts}

\part[1] Write the joint probability distribution $P(X_1, X_2, X_3, X_4, X_5)$, factorized as much as possible, using the conditional independence assumptions expressed by the above network.

\begin{your_solution}[height=2cm, width=\textwidth]
\end{your_solution}

\part[1] How many parameters would we need to represent the joint distribution \textbf{without} the conditional independence assumptions expressed by the Bayesian Network?

\begin{your_solution}[height=2cm, width=4cm]
\end{your_solution}

\part[1] How many parameters would we need to represent the joint distribution \textbf{with} the conditional independence assumptions expressed by the Bayesian Network?

\begin{your_solution}[height=2cm, width=4cm]
\end{your_solution}

\part[1] Which variables are in the Markov boundary of $X_4$? Note that the Markov boundary is the smallest possible Markov blanket.

\begin{your_solution}[height=2cm, width=4cm]
\end{your_solution}

\part By sampling from the network, we are able to estimate the following conditional probability tables: 

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
    \hline
         $X_1=0$ & $0.3$  \\ \hline
         $X_1=1$ & $0.7$    \\  \hline
    \end{tabular}
    \\[0.1 in] 
    \begin{tabular}{|c|c|c|}
    \hline
            & $X_1=0$ & $X_1=1$   \\  \hline
        $X_2=0$ & $0.8$ & $0.25$ \\  \hline
        $X_2=1$ & $0.2$  & $0.75$    \\  \hline
    \end{tabular}
    \\[0.1 in] 
    \begin{tabular}{|c|c|c|}
    \hline
            & $X_2=0$ & $X_2=1$   \\  \hline
        $X_3=0$ & $0.5$ & $0.6$ \\  \hline
        $X_3=1$ & $0.5$  & $0.4$    \\  \hline
    \end{tabular}
    \\[0.1 in] 
    \begin{tabular}{|c|c|c|}
    \hline
            & $X_2=0$ & $X_2=1$   \\  \hline
        $X_4=0$ & $0.3$ & $0.2$ \\  \hline
        $X_4=1$ & $0.7$  & $0.8$    \\  \hline
    \end{tabular}
    \\[0.1 in] 
    \begin{tabular}{|c|c|c|c|c|}
    \hline
            & $X_3=0, X_4=0$ & $X_3=0,X_4=1$ & $X_3=1,X_4=0$ & $X_4=1,X_3=1$   \\  \hline
        $X_5=0$ & $0.4$ & $0.7$ & $0.8$ & $0.5$ \\  \hline
        $X_5=1$ & $0.6$  & $0.3$ & $0.2$ & $0.5$    \\  \hline
    \end{tabular}
    \caption{Estimated Conditional Probability Tables}
\end{table}

In these tables, the columns represent the condition; for example, the top left entry in the last table corresponds to $P(X_5 = 0 \mid X_3 = 0, X_4 = 0)$. Using the values in Table 1, compute the following probabilities. Round to \textbf{four decimal places} after the decimal point. 

\begin{subparts}

\subpart[2] $P(X_1=0, X_2=1, X_3=0, X_4=1, X_5=0)$

\begin{your_solution}[title=Answer,height=2cm,width=3cm]
    % YOUR ANSWER 
    \end{your_solution}
    \begin{your_solution}[title=Work,height=4cm,width=12cm]
    % YOUR ANSWER 
\end{your_solution}

\subpart[2] $P(X_3 = 1)$ 

\begin{your_solution}[title=Answer,height=2cm,width=3cm]
    % YOUR ANSWER 
    \end{your_solution}
    \begin{your_solution}[title=Work,height=8cm,width=12cm]
    % YOUR ANSWER 
\end{your_solution}

\subpart[2] $P(X_1 = 1 | X_3 = 1)$ 

\begin{your_solution}[title=Answer,height=2cm,width=3cm]
    % YOUR ANSWER 
    \end{your_solution}
    \begin{your_solution}[title=Work,height=4cm,width=12cm]
    % YOUR ANSWER 
\end{your_solution}
\end{subparts}

\end{parts}

\clearpage 
\sectionquestion{Hidden Markov Models}


\begin{parts}
\part[2] \sall Let $Y_t$ be the state at time $t$. Which of the following are true under the (first-order) Markov assumption in an HMM?

\checkboxchar{$\Box$} \checkedchar{$\blacksquare$}
\begin{checkboxes}
    \choice The states are independent
    \choice The observations are independent
    \choice $Y_t \independent Y_{t-1} \mid Y_{t-2}$ 
    \choice $Y_t \independent Y_{t-2} \mid Y_{t-1}$ 
    \choice None of the above
\end{checkboxes}
    

\part[2] \sall Which of the following independence assumptions hold in an HMM?
\checkboxchar{$\Box$} \checkedchar{$\blacksquare$}
    \begin{checkboxes}
        \choice The current observation $x_t$ is conditionally independent of all other observations given the current state $Y_t$ 
        \choice The current observation $x_t$ is conditionally independent of all other states given the current state $Y_t$ 
        \choice The current state $Y_t$ is conditionally independent of all states given the previous state
        \choice The current observation $x_t$ is conditionally independent of $Y_{t-2}$ given the previous observation $x_{t-1}$ 
        \choice None of the above
    \end{checkboxes}


\newpage
 
\part In the remaining questions, you will see two quantities and decide which relations could hold between them. 

As a reminder, $\alpha_t(s_j) = P(Y_t=s_j, x_{1:t})$ and $\beta_t(s_j) = P(x_{t+1:T}\mid Y_t=s_j)$. Assume that there are $T$ observations and $T \geq 5$, $J$ possible hidden states so each $Y_t \in \{s_1, \ldots, s_J\}$, and that we do not use pseudocounts. Pseudocounts are explained in Section \ref{learn}.


\begin{subparts}
\subpart[2] \sall Which of the following are possible relations between $\sum_{i=1}^{J}(\alpha_5(s_i)\beta_5(s_i))$ and $P(x_{1:T})$?
\begin{checkboxes}
    \choice $=$
    \choice $>$
    \choice $<$
\end{checkboxes}


\subpart[2] \sall Which of the following are possible relations between $P(Y_4=s_1, Y_5=s_2, x_{1:T})$ and $\alpha_4(s_1) \beta_5(s_2)$?

\begin{checkboxes}
    \choice $=$
    \choice $>$
    \choice $<$
\end{checkboxes}


\subpart[2] \sall Which of the following are possible relations between $\alpha_5(s_i)$ and $\beta_5(s_i)$? 
\begin{checkboxes}
    \choice $=$
    \choice $>$
    \choice $<$
\end{checkboxes}


\end{subparts}
\end{parts}

\clearpage

\sectionquestion{Forward-Backward Algorithm}
\label{hmm-problem}

The following questions should be completed before you start the programming component of this
assignment. To help you prepare to implement the forward-backward algorithm (see Section~\ref{forback} for a detailed explanation), we have provided a small example for you to work through by hand. This toy data set consists of a training set of three sequences with three unique words and two states, and a validation set with a single sequence composed of the same words occurring in the training set. 

\textbf{Training set:} 
\begin{verbatim}
you     D
eat     C
fish    D

you     D
fish    D
eat     C

eat     C
fish    D
\end{verbatim}

The training word sequences are:
\begin{align*}
    \mathbf{x}^{(1)} &= [\texttt{you eat fish}]^T \\
    \mathbf{x}^{(2)} &= [\texttt{you fish eat}]^T \\
    \mathbf{x}^{(3)} &= [\texttt{eat fish}]^T 
\end{align*}

And the corresponding tags are:
\begin{align*}
    \mathbf{y}^{(1)} &= [D~~C~~D]^T \\
    \mathbf{y}^{(2)} &= [D~~D~~C]^T \\
    \mathbf{y}^{(3)} &= [C~~D]^T
\end{align*}

\textbf{Validation set:}
\begin{verbatim}
fish
eat
you
\end{verbatim}

The validation word sequence is: 
$$
\mathbf{x}_{\text{validation}}= 
\begin{bmatrix}
\texttt{fish eat you}\\
\end{bmatrix}^T
$$

\clearpage

In this section and the following section (Viterbi Decoding), we define:
\begin{itemize}
    \item Observations $x_t \in \{\texttt{you}, \texttt{eat}, \texttt{fish}\}$
    \item States $Y_t$ $\in \{C,D\}$
    \item $\mathbf{B}$ is the transition matrix, where $B_{jk} = P(Y_{t} = s_{k} \mid Y_{t-1} = s_{j})$. Here $\mathbf{B}$ is a $4 \times 4$ matrix, as we are also accounting for the START/END states.  
    \item $\mathbf{A}$ is the emission matrix, where $A_{jk} = P(X_{t} = o_k \mid Y_{t} = s_{j})$. Here $\mathbf{A}$ is a $4 \times 3$ matrix, as we are also accounting for the START/END states. 
    % As an example, $A_{23}$ denotes $P(X_{t} = 3 \mid Y_{t} = s_{2})$, or the probability that $X_{t}$ corresponds to \texttt{fish} given the hidden state $Y_t = D$.
\end{itemize} 

\textit{Note:} Pseudocounts (see Section \ref{learn}) should be used to estimate $\mathbf{B}$ and $\mathbf{A}$. We will also adhere to the conditions listed in Section \ref{learn} for entries in $\mathbf{B}$ and $\mathbf{A}$ involving the START/END states; these entries will be filled in for you and \textbf{do not require pseudocounts/should not be normalized}.

For all numerical answers, round to \textbf{four decimal places} after the decimal point. Showing your work in these questions is optional, but it is recommended to help us understand where any misconceptions may occur.

\begin{parts}
    \part[4] Fill in the following tables with the estimates of $\mathbf{B}$ (left) and $\mathbf{A}$ (right).
    \begin{table}[h]
    \parbox{.55\linewidth}{
    \center
    \begin{tabular}{|m{1.25cm}|m{1.25cm}|m{1cm}|m{1cm}|m{1cm}|}
    \hline
     & START & $C$ & $D$ & END \\ \hline
    START & 0 & ?? & ?? & ?? \\ \hline
    $C$ & 0 & ?? & ?? & ?? \\ \hline
    $D$ & 0 & ?? & ?? & ?? \\ \hline
    END & 0 & 0 & 0 & 0 \\ \hline 
    \end{tabular}}
    \parbox{.4\linewidth}{
    \center
    \begin{tabular}{|m{1.25cm}|m{1cm}|m{1cm}|m{1cm}|}
    \hline
     & \texttt{you} & \texttt{eat} & \texttt{fish} \\ \hline
    START & 1 & 1 & 1 \\ \hline
    $C$ & ?? & ?? & ?? \\ \hline
    $D$ & ?? & ?? & ?? \\ \hline
    END & 1 & 1 & 1 \\ \hline
    \end{tabular}}
    \end{table}
    
    
    \part[2] Compute $\alpha_1(C)$ and $\alpha_1(D)$, the $\alpha$ values associated with states $C$ and $D$ for the first word in the validation sequence. 
    
    \begin{your_solution}[title=$\alpha_1(C)$,height=2cm,width=3cm]
    % YOUR ANSWER 
    \end{your_solution}
    \begin{your_solution}[title=Work,height=2cm,width=12cm]
    % YOUR ANSWER 
    \end{your_solution}
    
    \begin{your_solution}[title=$\alpha_1(D)$,height=2cm,width=3cm]
    % YOUR ANSWER 
    \end{your_solution}
    \begin{your_solution}[title=Work,height=2cm,width=12cm]
    % YOUR ANSWER 
    \end{your_solution}
    
    
    \part[2] Compute $\alpha_2(C)$, the $\alpha$ value associated with state $C$ for the second word in the validation sequence. 
    
    \begin{your_solution}[title=$\alpha_2(C)$,height=2cm,width=3cm]
    % YOUR ANSWER 
    \end{your_solution}
    \begin{your_solution}[title=Work,height=5cm,width=12cm]
    % YOUR ANSWER 
    \end{your_solution}
    
    \part[2] Compute $\beta_2(D)$, the $\beta$ value associated with state $D$ for the second word in the validation sequence. 
    
    \begin{your_solution}[title=$\beta_2(D)$,height=2cm,width=3cm]
    % YOUR ANSWER 
    \end{your_solution}
    \begin{your_solution}[title=Work,height=5cm,width=12cm]
    % YOUR ANSWER
    \end{your_solution}    
    
    \clearpage
    \part[3] Predict the state for the third word in the validation sequence. 
    
    \begin{your_solution}[title=State,height=2cm,width=3cm]
    % YOUR ANSWER 
    \end{your_solution}
    \begin{your_solution}[title=Work,height=10cm,width=12cm]
    % YOUR ANSWER 
    \end{your_solution}  
    
    \part[2] Compute the log-likelihood for the entire validation sequence, ``\texttt{fish eat you}."
    
    \begin{your_solution}[title=Likelihood,height=2cm,width=3cm]
    % YOUR ANSWER 
    \end{your_solution}
    \begin{your_solution}[title=Work,height=4cm,width=12cm]
    % YOUR ANSWER 
    \end{your_solution} 

\end{parts}

\clearpage 

\sectionquestion{Viterbi Decoding}

In the Viterbi algorithm, we seek to find the most probable hidden state sequence $\hat{Y_1}, \dots, \hat{Y_T}$ given the observations $x_1,\dots,x_T$. 

We define:
\begin{itemize}
    % \item $\mathbf{B}$ to be the transition matrix: $B_{jk} = P(Y_{t} = s_{k} \mid Y_{t-1} = s_{j})$
    % \item $\mathbf{A}$ to be the emission matrix: $A_{jk} = P(X_{t} = k \mid Y_{t} = s_{j})$
    % \item $\boldsymbol{\pi}$ to be the initialization matrix for $Y_1$: $\pi_j = P(Y_1 = s_j)$
    \item $\omega_t(s_k)$ to be the probability of most probable sequence of $t$ states ending at state $s_k$, given the first $t$ observations:
    \begin{align}
    \omega_t(s_k) = \max_{y_1,\dots, y_{t-1}} P(x_{1:t},y_{1:t-1},Y_t=s_k)    
    \end{align}
    \item $b_t(s_k)$ to be the backpointer that stores the path through hidden states that gives us the highest probability:
    \begin{align}
    b_t(s_k) = \argmax_{y_1,\dots, y_{t-1}} P(x_{1:t},y_{1:t-1},Y_t=s_k)    
    \end{align}
\end{itemize}

We outline the Viterbi Algorithm below:
\begin{enumerate}
    % \item Initialize $\omega_1(s_j) = \pi_{j} A_{jx_{1}}$ and $b_1(s_j) = j$
    \item Initialize $\omega_0(\text{START}) = 1$
    \item For $1 \leq t \leq T + 1$, we calculate
    \begin{align*}
        \omega_t(s_j) &= \max_{k \in \{1,\dots,J\}} A_{j x_{t}} B_{kj} \omega_{t-1}(s_k) \\
        b_t(s_j) &= \argmax_{k \in \{1,\dots,J\}} A_{j x_{t}} B_{kj} \omega_{t-1}(s_k)
    \end{align*}
\end{enumerate}

We can obtain the most probable sequence by backtracing through the backpointers as follows:
\begin{enumerate}
    % \item $\hat{y}_T = \argmax_{k \in \{1,\dots,J\}} \omega_T(s_k)$. 
    \item $\hat{Y}_T = b_{T+1}(END)$
    \item For $t = T-1,\dots,1$: \\
    $~~~~~~~\hat{Y}_{t} = b_{t+1}(\hat{Y}_{t+1})$
    \item Return $\hat{Y}_1, \dots, \hat{Y}_T$
\end{enumerate}

For the following questions, consider the Hidden Markov Model specified below and assume we observe the sequence $\mathbf{x} = [\texttt{you eat}]^T$.

% \begin{center}
% \begin{tikzpicture}[node distance=2cm,->,>=latex,auto,
%   every edge/.append style={thick}]
%   \node[state] (1) {$C$};
%   \node[state] (2) [right of=1] {$D$};  
%   \path (1) edge[loop left]  node{$0.7$} (1)
%             edge[bend left]  node{$0.3$} (2)
%         (2) edge[loop right]  node{$0.6$} (2)
%             edge[bend left]  node{$0.4$} (1);
% \end{tikzpicture}
% \end{center}

% \begin{table}[h!]
% \centering
% \begin{tabular}{|l|l|l|}
% \hline
% \multicolumn{1}{|c|}{$k$} & \multicolumn{1}{c|}{$P(X_t=k\mid Y_t=C)$} & $P(X_t=k \mid Y_t=D)$ \\ \hline
% $o_1$ & 0.5 & 0.3 \\ \hline
% $o_2$ & 0.4 & 0.5 \\ \hline
% $o_3$ & 0.1 & 0.2 \\ \hline
% \end{tabular}
% \caption{}
% \end{table}

\begin{table}[h]
    \parbox{.6\linewidth}{
    \center
    \begin{tabular}{|m{1.5cm}|m{1cm}|m{1cm}|m{1cm}|}
    \hline
     & $C$ & $D$ & END \\ \hline
    START & 0.5 & 0.5 & 0 \\ \hline
    $C$ & 0.5 & 0.3 & 0.2 \\ \hline
    $D$ & 0.3 & 0.6 & 0.1 \\ \hline
    \end{tabular}}
    \parbox{.35\linewidth}{
    \center
    \begin{tabular}{|m{1cm}|m{1cm}|m{1cm}|m{1cm}|}
    \hline
     & \texttt{you} & \texttt{eat} & \texttt{fish} \\ \hline
    $C$ & 0.5 & 0.4 & 0.1 \\ \hline
    $D$ & 0.3 & 0.5 & 0.2 \\ \hline
    \end{tabular}}
\end{table}
Note that the values in these matrices aren't necessarily the same as those you estimated in the previous section. Showing your work in these questions is optional, but it is recommended to help us understand where any misconceptions may occur. Only your answer in the left box will be graded.

\clearpage
\begin{parts}
    \part[2] Compute $\omega_1(C)$ and $\omega_1(D)$. Round to \textbf{four decimal places} after the decimal point.
    

    \begin{your_solution}[title=$\omega_1(C)$,height=2cm,width=3cm]
    % YOUR ANSWER 
    \end{your_solution}
    \begin{your_solution}[title=Work,height=2cm,width=12cm]
    % YOUR ANSWER 
    \end{your_solution}
    
    \begin{your_solution}[title=$\omega_1(D)$,height=2cm,width=3cm]
    % YOUR ANSWER 
    \end{your_solution}
    \begin{your_solution}[title=Work,height=2cm,width=12cm]
    % YOUR ANSWER 
    \end{your_solution}
    
    
    \part[2] Compute $\omega_2(C)$ and $\omega_2(D)$. Round to \textbf{four decimal places} after the decimal point.

    \begin{your_solution}[title=$\omega_2(C)$,height=2cm,width=3cm]
    % YOUR ANSWER 
    \end{your_solution}
    \begin{your_solution}[title=Work,height=4cm,width=12cm]
    % YOUR ANSWER 
    \end{your_solution}
    
    \begin{your_solution}[title=$\omega_2(D)$,height=2cm,width=3cm]
    % YOUR ANSWER 
    \end{your_solution}
    \begin{your_solution}[title=Work,height=4cm,width=12cm]
    % YOUR ANSWER 
    \end{your_solution}

    \part[2] \sone Which of the following is the most likely sequence of hidden states given the observed sequence? 
    
    \begin{checkboxes}
        \item $Y_1=C$, $Y_2=C$ 
        \item $Y_1=D$, $Y_2=D$ 
        \item $Y_1=D$, $Y_2=C$
        \item $Y_1=C$, $Y_2=D$ 
        \item Not enough information. 
    \end{checkboxes}
 
\end{parts}

\clearpage

\sectionquestion{Forward-Backward Algorithm Vectorization}

The forward-backward algorithm is a dynamic programming approach to efficiently compute forward and backward probabilities for a given sequence. To help with your implementation of the algorithm in the programming section, in this section you will derive vectorized expressions for the forward and backward probabilities in log space. For an explanation of working in log space and the log-sum-exp trick, see Section \ref{sec:underflow} and HW7 Recitation. 

Express the logarithms of the following alpha vectors $\alphav_i$ and beta vectors $\betav_i$ in terms of $\xv$, $\log\Av$, $\log\Bv$ (indexing them as appropriate), and $\log\alphav_t$ and $\log\betav_t$ for any $t \neq i$. Your solutions must rely on matrix operations (e.g. matrix multiplication, elementwise multiplication $\odot$, elementwise addition $\oplus$). You may use the \texttt{LogSumExp} function, and assume it is applied row-wise for a matrix (yielding a column vector as a result). You may also assume NumPy-style broadcasting and that all $\alphav_i$ and $\betav_i$ are column vectors. 
    \begin{subparts}
    \subpart[1]$\log(\alphav_1)$\\
    \begin{your_solution}[height=4cm]
    % YOUR ANSWER 
    \end{your_solution} 
    
    \subpart[2]$\log(\alphav_2)$\\
    \begin{your_solution}[height=4cm]
        % YOUR ANSWER 
    \end{your_solution} 
    
    \subpart[2]$\log(\betav_2)$\\
    \begin{your_solution}[height=4cm]
        % YOUR ANSWER 
    \end{your_solution} 
    \end{subparts}

\clearpage

\sectionquestion{Empirical Questions}

Return to these questions after implementing your \texttt{learnhmm.py} and \\ \texttt{forwardbackward.py} functions. Please ensure that you have used the log-sum-exp trick in your programming as described in Section \ref{sec:underflow} before answering these empirical questions.

Using the full data set \textbf{en\_data/train.txt} in the handout, use your implementation of\\ \texttt{learnhmm.py} to learn HMM parameters using the first 10, 100, 1000, and 10000 sequences in the file.
Use these learned parameters to perform prediction on both the English \textbf{train.txt} and the \textbf{validation.txt} files with your implementation of \texttt{forwardbackward.py}.
Construct a plot with number of sequences used for training on the x-axis (log-scale with base $e$) and average log likelihood across all sequences from the English \textbf{train.txt} and the \textbf{validation.txt} on the y-axis (see Section~\ref{forback} for details on computing the log data likelihood for a sequence).

Fill in the table with the resulting log likelihood values, rounded to two decimal places, and 
include your plot in the large box.
To receive credit for your plot, you must submit a computer generated plot.
\textbf{DO NOT} hand draw your plot.

\begin{parts}
\part[4] Fill in this table with your computed log-likelihood values. 

% YOUR ANSWER 
% Fill in the table by replacing the ??s with the appropriately calculated likelihood
\begin{table}[h]
    \center
    \begin{tabular}{|m{2cm}|m{6cm}|m{6cm}|}
    \hline
    \# Sequences & Train Average Log-Likelihood & Validation Average Log-Likelihood \\ \hline
    10         &  ?? & ?? \\ \hline
    100        &  ?? & ?? \\ \hline
    1000       &  ?? & ?? \\ \hline
    10000      &  ?? & ?? \\ \hline
    \end{tabular}
    \end{table}
    
    \end{tabular}
\end{table}

\part[2] Put your plot below:

    \begin{your_solution}[title=Plot,height=10cm,width=16cm]
        % YOUR ANSWER 
        % \begin{center}
        %    \includegraphics[height8.5cm]{allplot.png}
        % \end{center}


    \end{your_solution}



\end{parts}

\clearpage
\newpage
\end{questions}
\newpage
\section{Collaboration Questions}
After you have completed all other components of this assignment, report your answers to these questions regarding the collaboration policy. Details of the policy can be found \href{https://www.cs.cmu.edu/~hchai2/courses/10601/#Syllabus}{here}.
\begin{enumerate}
    \item Did you receive any help whatsoever from anyone in solving this assignment? If so, include full details.
    \item Did you give any help whatsoever to anyone in solving this assignment? If so, include full details.
    \item Did you find or come across code that implements any part of this assignment? If so, include full details.
\end{enumerate}

\begin{your_solution}[height=6cm]
% YOUR ANSWER 

\end{your_solution}

\newpage
\section{Programming (90 points)}
\label{programming}

\subsection{The Task}\label{task}
In the programming section you will implement a named entity recognition system using Hidden Markov Models (HMMs). Named entity recognition (NER) is the task of classifying named entities, typically proper nouns,  into pre-defined categories, such as person, location, or organization. Consider the example sequence below, where each word is appended with a tab and then its tag:

\begin{tabular}{ m{3cm}  m{3cm} } 
    `` & O \\
    Rhinestone & B-ORG \\
    Cowboy & I-ORG \\
    '' & O \\
    ( & O \\
    Larry & B-PER \\
    Weiss & I-PER \\
    ) & O \\ 
    - & O \\
    3:15 & O
\end{tabular}

 \texttt{Rhinestone} and \texttt{Cowboy} are labeled as an organization (\texttt{ORG}), while \texttt{Larry} and \texttt{Weiss} is labeled as a person (\texttt{PER}). Words that aren't named entities are assigned the \texttt{O} tag. The \texttt{B-} prefix indicates that a word is the beginning of an entity, while the \texttt{I-} prefix indicates that the word is inside the entity.

NER is an incredibly important task for a machine to analyze and interpret a body of natural language text. For example, when designing a system that automatically summarizes news articles, it is important to recognize the key subjects in the articles. Another example is designing a trivia bot. If you can quickly extract the named entities from the trivia question, you may be able to more easily query your knowledge base (e.g. type a query into Google) to request information about the answer to the question.

On a technical level, the main task is to implement an algorithm to learn the HMM parameters given the training data and then implement the forward-backward algorithm to perform a smoothing query which we can then use to predict the hidden tags for a sequence of words. 

\subsection{The Dataset}\label{dataset}
\href{https://aclanthology.org/P17-1178.pdf}{WikiANN} is a ``silver standard" dataset that was generated without human labelling. The English Abstract Meaning Representation (AMR) corpus and DBpedia features were used to train an automatic classifier to label Wikipedia articles. These labels were then propagated throughout other Wikipedia articles using the Wikipedia's cross-language links and redirect links. Afterwards, another tagger that self-trains on the existing tagged entities was used to label all other mentions of the same entities, even those with different morphologies (prefixes and suffixes that modify a word in other languages). Finally, the amassed training examples were filtered by ``commonness" and ``topical relatedness" to pick more relevant training data. 

The WikiANN dataset provides labelled entity data for Wikipedia articles in 282 languages. We will be primarily using the English subset, which contains 14,000 training examples and 3,300 test examples, and the French subset, which contains around 7,500 training examples and 300 test examples. 

\clearpage

\subsection{File Formats}\label{formats}
The contents and formatting of each of the files in the handout folder is explained below. 
\begin{enumerate}

\item \textbf{train.txt} This file contains labeled text data that you will use in training your model in the Learning problem (Section \ref{learn}). Specifically, the text contains one word per line that has already been preprocessed, cleaned and tokenized. Every sequence has the following format:

    \texttt{<Word0>\textbackslash t<Tag0>\textbackslash n<Word1>\textbackslash t<Tag1>\textbackslash n ... <WordN>\textbackslash t<TagN>\textbackslash n}

where every \texttt{<WordK>\textbackslash t<TagK>} unit token is separated by a newline. Between each sequence is an empty line. If we have two three-word sequences in our data set, the data will look like so:

    \texttt{<Word0>\textbackslash t<Tag0>\textbackslash n\\<Word1>\textbackslash t<Tag1>\textbackslash n\\<Word2>\textbackslash t<Tag2>\textbackslash n\\\textbackslash n\\}
    \texttt{<Word0>\textbackslash t<Tag0>\textbackslash n\\<Word1>\textbackslash t<Tag1>\textbackslash n\\<Word2>\textbackslash t<Tag2>}
    
\textit{Note:} Word 2 of the second sequence does not end with a newline because it is the end of the data set.

Since the \texttt{<START>} and \texttt{<END>} tags do not have associated words, they do not appear in \textbf{train.txt}. The \texttt{parse\_args} function in the starter code automatically includes these special tags for you.

\item \textbf{validation.txt}: This file contains labeled validation data that you will use to evaluate your model. This file has the same format as \textbf{train.txt}.
    
\item \textbf{index\_to\_word.txt, index\_to\_tag.txt}: These files contain a list of all words or tags that appear in the data set. The format is simple:

\begin{tabular}{ m{5cm}  m{5cm} } 
    \textbf{index\_to\_word.txt} & \textbf{index\_to\_tag.txt} \\
    \texttt{<Word0>\textbackslash n} & \texttt{<Tag0>\textbackslash n} \\
    \texttt{<Word1>\textbackslash n} & \texttt{<Tag1>\textbackslash n} \\
    \texttt{<Word2>\textbackslash n} & \texttt{<Tag2>\textbackslash n} \\
    $\vdots$ & $\vdots$
\end{tabular}

In your functions, you will convert the string representation of words or tags to indices corresponding to the location of the word or tag in these files. For example, if \textit{Austria} is on line 729 of \textbf{index\_to\_word.txt}, then all appearances of \textit{Austria} in the data sets should be converted to the index 729.

\item \textbf{predicted.txt}: This file contains labeled data that you will use to debug your implementation. The labels in this file are generated by running a reference implementation using the features from \textbf{train.txt}. This file has the same format as \textbf{train.txt} (and therefore should not include the \texttt{<START>} and \texttt{<END>} tags).

\item \textbf{metrics.txt}: This file contains the metrics you will compute for the validation data. The first line should contain the average log likelihood, and the second line should contain the prediction accuracy. There should be a single space after the colon preceding the metric value; see the reference output file for more details.  

\item \textbf{hmmtrans.txt, hmmemit.txt}: These files contain pre-trained model parameters of an HMM that you can use to test your implementation of the Learning and Evaluation and Decoding problems (Sections \ref{learn}, \ref{forback}). Both files are formatted as space-separated matrices, where each entry corresponds to an entry in the transition or emission matrix. In the case of transition probabilities, the entries correspond to the probability of transitioning from one tag to another. Similarly, in the case of emission probabilities, the entries correspond to the probability of emitting a particular word, given a current tag. Elements in the same row are separated by a space. Each row corresponds to a line of text, using \texttt{\char`\\ n} to create new lines.
    
\textbf{hmmtrans.txt}:

\texttt{<ProbS1S1> <ProbS1S2> ... <ProbS1SN>\textbackslash n}\\
\texttt{<ProbS2S1> <ProbS2S2> ... <ProbS2SN>\textbackslash n}...

Above, the entry \texttt{<ProbS2S1>} corresponds to the probability $P(Y_t = s_1 | Y_{t-1} = s_2)$, where $Y_i$ are tags.

\textbf{hmmemit.txt}:

\texttt{<ProbS1Word1> <ProbS1Word2> ... <ProbS1WordN>\textbackslash n}\\
\texttt{<ProbS2Word1> <ProbS2Word2> ... <ProbS2WordN>\textbackslash n}...

Above, the entry \texttt{<ProbS2Word1>} corresponds to the probability $P(X_t = x_1 | Y_t = s_2)$, where $X_t$ is a word and $Y_t$ is a tag.


\end{enumerate}

\subsection{Learning}\label{learn}
Your first task is to write a program \texttt{learnhmm.py} to learn the Hidden Markov Model parameters needed to apply the forward-backward algorithm (See Section \ref{forback}). You will need to estimate the transition probabilities $\mathbf B$ and the emission probabilities $\mathbf A$. For this assignment, we model each of these probabilities using a multinomial distribution with parameters $ B_{jk} = P(Y_{t}=s_k\mid Y_{t-1}=s_j)$, and $ A_{jk} = P(X_t=x_k\mid Y_{t}=s_j)$. These can be estimated using maximum likelihood, which results in the following parameter estimates:

\begin{enumerate}
    \item $P(Y_{t} = s_k\mid Y_{t-1}=s_j) = B_{jk}= \frac{1+N_{Y_t=s_k,Y_{t-1}=s_j}}{\sum_{p=1}^J (1+N_{Y_t=s_p,Y_{t-1}=s_j})}$, where $N_{Y_t=s_k,Y_{t-1}=s_j}$ is the number of times tag $s_j$ is followed by tag $s_k$ in the training data set.
    \item $P(X_{t} = x_k\mid Y_{t}=s_j) = A_{jk}= \frac{1+N_{X_t=x_k,Y_t=s_j}}{\sum_{p=1}^M (1+N_{X_t=x_p,Y_t=s_j})}$, where $N_{X_t=x_k,Y_t=s_j}$ is the number of times that the tag $s_j$ emits the word $x_k$ in the training data set.
\end{enumerate}

Note we add 1 to each count to make a pseudocount. This is slightly different from pure maximum likelihood estimation, but it is useful in improving performance when evaluating unseen transitions or emissions on the validation set.

We include \textit{both} the special tags \texttt{<START>} and \texttt{<END>} in the $\mathbf{A}$ and $\mathbf{B}$ matrices, which makes indexing easier. As a result, after learning $\mathbf{A}$ and $\mathbf{B}$, you must ensure that the following conditions hold:
\begin{enumerate}
    \item $P(Y_t = \texttt{<START>} | Y_{t-1} = s_j) = B_{j, \hspace{2pt}\texttt{<START>}} = 0$ for all $j$, since nothing can transition to $\texttt{<START>}$.
    \item $P(Y_t = s_k | Y_{t-1} = \texttt{<END>}) = B_{\texttt{<END>}, \hspace{2pt}k} = 0$ for all $k$, since $\texttt{<END>}$ cannot transition to any other tag. This implies that the row in $\mathbf{B}$ corresponding to the $\texttt{<END>}$ tag is \textit{not} a probability distribution.
    \item $P(X_t = x_k | Y_t = \texttt{<START>}) = A_{\texttt{<START>}, k} = P(X_t = x_k | Y_t = \texttt{<END>}) = A_{\texttt{<END>}, k} = 1$ for all $k$. While setting these values to $0$ may be more natural since the special \texttt{<START>} and \texttt{<END>} tags never emit anything, setting these values to 1 allows us to explicitly consider fewer edge cases in the forward-backward algorithm and write cleaner (and equally correct) code.
\end{enumerate}

Your implementation should read in the training data set (\textbf{train.txt}) and then estimate $\mathbf{B}$ and $\mathbf{A}$ using the above maximum likelihood solutions with pseudocounts.

Your outputs should be in the same format as \textbf{hmmtrans.txt} and \textbf{hmmemit.txt} (including the same number of decimal places to ensure there are no rounding errors during prediction). The autograder runs and evaluates the output from the files generated, using the following command:

\begin{tabbing}
\=\texttt{\$ \textbf{python} learnhmm.\textbf{py} [args\dots]}\\
\end{tabbing}

Where \texttt{[args\dots]} is a placeholder for five command-line arguments: \texttt{<train\_input>} \texttt{<index\_to\_word>} \texttt{<index\_to\_tag>} \texttt{<hmmemit>} \texttt{<hmmtrans>}. These arguments are described below:
\begin{enumerate}
    \item \texttt{<train\_input>}: path to the training input \texttt{.txt} file (see Section~\ref{dataset})
    \item \texttt{<index\_to\_word>}: path to the \texttt{.txt} file that specifies the dictionary mapping from words to indices. The tags are ordered by index, with the first word having index of 0, the second word having index of 1, etc.
    \item \texttt{<index\_to\_tag>}: path to the \texttt{.txt} file that specifies the dictionary mapping from tags to indices. The tags are ordered by index, with the first tag having index of 0, the second tag having index of 1, etc.
    \item \texttt{<hmmemit>}: path to output \texttt{.txt} file to which the emission probabilities ($\mathbf A$) will be written. The file output to this path should be in the same format as the handout \texttt{hmmemit.txt} (see Section~\ref{dataset})
    \item \texttt{<hmmtrans>}: path to output \texttt{.txt} file to which the transition probabilities ($\mathbf B$) will be written. The file output to this path should be in the same format as the handout \texttt{hmmtrans.txt} (see Section~\ref{dataset}).
\end{enumerate}
\vspace{0.2 in}
As an example, the following command would run your program on the toy dataset provided in the
handout.
\begin{lstlisting}
$ python learnhmm.py toy_data/train.txt toy_data/index_to_word.txt \ 
toy_data/index_to_tag.txt hmmemit.txt hmmtrans.txt
\end{lstlisting}

After running the command above, the \textbf{hmmemit.txt} and \textbf{hmmtrans.txt} output files should match the reference files provided in the \texttt{toy\_output} directory.

\newpage

\subsection{Evaluation and Decoding}
\label{forback}

\subsubsection{Forward Backward Algorithm and Minimal Bayes Risk Decoding}

Your next task is to implement the forward-backward algorithm. Suppose we have a set of sequence consisting of $T$ words, $x_1,\dots,x_T$. Each word is associated with a tag $Y_t\in\{1,\dots,J\}$ (including the \texttt{<START>} and \texttt{<END>} tags). In the forward-backward algorithm we seek to compute $P(Y_t \mid x_{1:T})$ for each $t$ up to a multiplicative constant. This is done by first breaking $P(Y_t \mid x_{1:T})$ into a ``forward'' component and a ``backward'' component as follows:
\begin{align*}
   P(Y_t =s_j \mid x_{1:T}) &\propto P(Y_t=s_j,x_{t+1:T} \mid x_{1:t})\\
   &\propto P(Y_t=s_j \mid x_{1:t})P(x_{t+1:T} \mid Y_t=s_j, x_{1:t})\\
    &\propto P(Y_t=s_j \mid x_{1:t})P(x_{t+1:T} \mid Y_t=s_j)\\ 
     &\propto P(Y_t=s_j, x_{1:t})P(x_{t+1:T} \mid Y_t=s_j) 
\end{align*}

Then, we can efficiently compute $P(Y_t=s_j, x_{1:t})$ and $P(x_{t+1:T} \mid Y_t=s_j)$ using dynamic programming.

\textbf{Forward Algorithm}

Define $\alpha_t(s_j) = P(Y_t = s_j, x_{1:t})$. This can be rearranged into the following expression:
\begin{align}
    \label{eqn:alpha}
    \alpha_t(s_j) = P(x_t | Y_t = s_j) \sum_{k=1}^{J} P(Y_t = s_j | Y_{t-1} = s_k) \alpha_{t-1} (s_k)
\end{align}

Using this definition, the $\alpha$'s can be computed using the following dynamic programming procedure:

\begin{lstlisting}[language=Python,escapechar=@]
@$\alpha_0 (\texttt{<START>}) = 1$@
@$\alpha_0 (s_m) = 0$ \text{for all} $s_m \neq \texttt{<START>}$@
for t = 1,...,T:
    for j = 1,...,J:
        @$\alpha_t(s_j) = A_{j, \hspace{1pt}x_t}  \sum_k(B_{k, j}\alpha_{t-1}(s_k))$@
\end{lstlisting}

\textbf{Backward Algorithm}

Define $\beta_t(s_j) = P(x_{t+1:T} \mid Y_t=s_j)$. This can be rearranged into the following expression:
\begin{align}
    \label{eqn:beta}
    \beta_t(s_j) &= \sum_{k=1}^J \beta_{t+1}(s_k) P(x_{t+1} | Y_{t + 1} = s_k) P(Y_{t + 1} = s_k | Y_t = s_j)
\end{align}


Just like the $\alpha$'s, the $\beta$'s can also be computed using the following dynamic programming procedure:

\begin{lstlisting}[language=Python,escapechar=@]
@$\beta_{T+1}(\texttt{<END>}) = 1$@
@$\beta_{T+1}(s_m) = 0$ for all $s_m \neq \texttt{<END>}$@
for t = T,...,1:
    for j = 1,...,J:
        @$\beta_t(s_j) = \sum_k(\beta_{t+1}(s_k)A_{k,x_{t+1}}B_{j,k})$@
\end{lstlisting}
Note that, for both the forward and backward algorithms, $A_{k, x_{T+1}} = 1$ where $k = \texttt{<END>}$, as defined in section \ref{learn}.

\clearpage
\textbf{Forward-Backward Algorithm}
As stated above, the goal of the Forward-Backward algorithm is to compute $P(Y_t =s_j \mid x_{1:T})$ up to a constant factor. This can be done using the following relation:

$$P(Y_t =s_j \mid x_{1:T}) \propto P(Y_t=s_j, x_{1:t})P(x_{t+1:T} \mid Y_t=s_j) $$

After running your forward and backward passes through the sequence, the conditional probabilities (up to a constant factor) are:

$$P(Y_t \mid x_{1:t}) \propto \alpha_t\odot\beta_t$$
where $\odot$ is the element-wise product.

\vspace{0.4 in}
\textbf{Minimum Bayes Risk Prediction}
We will assign tags using the minimum Bayes risk predictor, defined for this problem as follows:

$$\hat{Y}_t = \argmax_{j\in \{1,\dots,J\}} P(Y_t = s_j \mid x_{1:T})$$

To resolve ties, select the tag that appears earlier in the \textbf{index\_to\_tag.txt} input file.

\textbf{Computing the Log Likelihood of a Sequence}
When we compute the log likelihood of a sequence, we are interested in the computing the quantity $\log(P(x_{1:T}))$. We can rewrite this in terms of values we have already computed in the forward-backward algorithm as follows:

\begin{align*}
    \log{P(x_{1:T})} &= \log{\left(\sum_j P(x_{1:T},Y_t=s_j)\right)}\\
    &= \log{\left(\sum_j P(Y_t = s_j, x_{1:t}) P(x_{t+1:T} | Y_t = s_j) \right)} \\
    &= \log{\left(\sum_j \alpha_t (s_j) \beta_t (s_j)\right)}
\end{align*}

In the formula above, note that the log-likelihood is the same for any choice of $t$; it can therefore be helpful to choose a convenient value of $t$ that reduces computation.

\clearpage
\subsubsection{Implementation Details}

You should now write a program \texttt{forwardbackward.py} that implements the forward-backward algorithm. The program will read in validation data and the parameter files produced by \texttt{learnhmm.py}. The autograder runs and evaluates the output from the files generated, using the following command:

\begin{tabbing}
\=\texttt{\$ \textbf{python} forwardbackward.\textbf{py} [args\dots]}\\
\end{tabbing}

Where \texttt{[args\dots]} is a placeholder for seven command-line arguments:\texttt{<validation\_input>} \texttt{<index\_to\_word>} \texttt{<index\_to\_tag>} \texttt{<hmmemit>} \texttt{<hmmtrans>} \texttt{<predicted\_file>} \texttt{<metric\_file>}. These arguments are described in detail below:
\begin{enumerate}
    \item \texttt{<validation\_input>}: path to the validation input \texttt{.txt} file that will be evaluated by your forward backward algorithm (see Section~\ref{dataset})
    \item \texttt{<index\_to\_word>}: path to the \texttt{.txt} file that specifies the dictionary mapping from words to indices. The tags are ordered by index, with the first word having index of 0, the second word having index of 1, etc. This is the same file as was described for \texttt{learnhmm.py}.
    \item \texttt{<index\_to\_tag>}: path to the \texttt{.txt} file that specifies the dictionary mapping from tags to indices. The tags are ordered by index, with the first tag having index of 0, the second tag having index of 1, etc. This is the same file as was described for \texttt{learnhmm.py}.
    \item \texttt{<hmmemit>}: path to input \texttt{.txt} file which contains the emission probabilities ($\mathbf A$).
    \item \texttt{<hmmtrans>}: path to input \texttt{.txt} file which contains transition probabilities ($\mathbf B$).
    \item \texttt{<predicted\_file>}: path to the output \texttt{.txt} file to which the predicted tags will be written. The file should be in the same format as the \texttt{<validation\_input>} file. 
    \item \texttt{<metric\_file>}: path to the output \texttt{.txt} file to which the metrics will be written. 
\end{enumerate}

As an example, the following command would run your program on the toy dataset provided in the handout.
\begin{lstlisting}
$ python3 forwardbackward.py toy_data/validation.txt \ 
toy_data/index_to_word.txt toy_data/index_to_tag.txt \
toy_output/hmmemit.txt toy_output/hmmtrans.txt \
predicted.txt metrics.txt
\end{lstlisting}

After running the command above, the \texttt{<predicted\_file>} output should be:

\begin{lstlisting}
fish	D
eat 	C
you 	D

\end{lstlisting}

\clearpage
And the \texttt{<metric\_file>} output should be:

% Change the log likelihood values to reflect last-minute refsol changes
\begin{lstlisting}
Average Log-Likelihood: -5.100082181210599
Accuracy: 0.3333333333333333
\end{lstlisting}

where average log-likelihood and accuracy are evaluated over the validation set.

Take care that your output has the exact same format as shown above. There should be a single space after the colon preceding the metric value (e.g. a space after \lstinline{Average Log-Likelihood:}). Each line should be terminated by a Unix line ending \lstinline{\n}.

\subsubsection{Log-Space Arithmetic for Avoiding Underflow}
\label{sec:underflow}

Handling underflow properly is a critical step in implementing an HMM. The most generalized way of handling numerical underflow due to products of small positive numbers (like probabilities) is to calculate everything in log-space, i.e., represent every quantity by their logarithm. 

For this homework, using log-space starts with transforming Eq.(\ref{eqn:alpha}) and Eq.(\ref{eqn:beta}) into logarithmic form - you may find the recitation handout helpful. Please use base $e$ (natural log) for logarithm calculation.

After transforming the equations into log form, you may discover calculations of the following type:

$$ \log \sum_i \exp{(v_i)}$$

This may be programmed as is, but $\exp{(v_i)}$ may cause underflow when $v_i$ is large and negative. One way to avoid this is to use the \href{https://www.xarg.org/2016/06/the-log-sum-exp-trick-in-machine-learning/}{log-sum-exp trick}. We provide the pseudocode for this trick in Algorithm \ref{alg:log-sum-exp-trick}:

\begin{algorithm}[H]
    \caption{Log-Sum-Exp Trick}
    \label{alg:log-sum-exp-trick}
    \begin{algorithmic}[1]
        \Procedure{LogSumExp}{($v_1, v_2, \cdots, v_n$)}
            \State $m = \max(v_i)$ for $i=\{1, 2,\cdots, n\}$ 
            \State \textbf{return }{$m + \log(\sum_i\exp(v_i-m))$} 
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\textbf{Note: The autograder test cases account for numerical underflow using the Log-Sum-Exp Trick. If you do not implement \textbf{forwardbackward.py} with the trick, you will only receive partial credit.}

\subsection{Starter Code}

To help you start this assignment, we have provided starter code in the handout.

\subsection{Gradescope Submission}

You should submit your \texttt{learnhmm.py} and \texttt{forwardbackward.py} to Gradescope. Please do not use other file names. This will cause problems for the autograder to correctly detect and run your code.



\end{document}
